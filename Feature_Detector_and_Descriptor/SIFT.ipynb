{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIFT (Scale-Invariant Feature Transform) #\n",
    "\n",
    "Feature detection and description.\n",
    "\n",
    "**+ Accurate**\n",
    "\n",
    "**- Slow**\n",
    "\n",
    "**- Non-free**\n",
    "\n",
    "__1. Scale-space extrema detection:__\n",
    "\n",
    "Use scale-space filtering to detect keypoint with different scales.\n",
    "\n",
    "Laplacian of Gaussian (LoG) acts as a blob detector in various sizes due to different $\\sigma$ but is costly.\n",
    "\n",
    "![](img/sift_dog.jpg)\n",
    "\n",
    "Thus, Difference of Gaussian (DoG) is used instead as an approximation of LoG. It is the difference of Gaussian blurring of an image with $\\sigma$ and $k\\sigma$. This process is applied for different octaves of the image in the Gaussian Pyramid.\n",
    "\n",
    "When DoG are found for each resolution (octave/pyramid level) and scale ($\\sigma$), images are searched for local extrema over scale and space. One pixel in an image is compared with its 8 neighbours, as well as 9 pixels in next scale, and 9 pixels in previous scale.\n",
    "\n",
    "Optimal parameters are:\n",
    "- number of octaves: 4\n",
    "- number of scale levels: 5\n",
    "- initial $\\sigma$: 1.6\n",
    "- k: $\\sqrt{2}$\n",
    "\n",
    "![](img/sift_local_extrema.jpg)\n",
    "\n",
    "__2. Keypoint Localization:__\n",
    "\n",
    "Potential keypoints are refined to eliminates low-contrast and edge keypoints. Taylor series expansion of scale space is used to get more accurate location of extrema. If the intensity at this extrema is less than a threshold, it is rejected.\n",
    "\n",
    "DoG as higher response for edges.A 2x2 Hessian matrix is used to compute the principal curvature. Eigen values are then checked based on Harris corner detector observations: ratio of eigen values is compared to a threshold, if it is greater the keypoint is an edge and is discarded.\n",
    "\n",
    "Thresholds:\n",
    "- contrast: 0.03\n",
    "- edge = 10\n",
    "\n",
    "__3. Orientation assignement:__\n",
    "\n",
    "A neighbourhood (size depending on the scale) is taken around the keypoint location, then the gradient magnitude and direction is calculated in that region.\n",
    "\n",
    "An orientation histogram of 36 bins covering 360Â° is created. It is weighted by gradient magnitude and gaussian-weighted circular window ($\\sigma = 1.5 \\times keypoint scale$).\n",
    "\n",
    "The highest peak in the histogram is taken and any peak above 80% of it is also considered to calculate the orientation.\n",
    "\n",
    "![](img/sift_orientation_hist.png)\n",
    "\n",
    "__4. Keypoint descriptor:__\n",
    "\n",
    "A 16x16 neighbourhood around keypoint is taken, divided into 16 4x4 sub-blocks. For each sub-block, a 8 bin orientation histogram is created, for a total of 128 bins.\n",
    "\n",
    "Then, it is represented as a vector to form keypoint descriptor.\n",
    "\n",
    "__5. Keypoint matching:__\n",
    "\n",
    "In some cases, the second closest-match may be very near to the first one. So, avery matches veryfying the following condition are rejected: $$\\frac{1^{st}closest-match}{2^{nd}closest-match} < 0.8$$\n",
    "\n",
    "It eliminates around 90% of false matches while discards only 5% correct matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # image_file = \"lena.png\"\n",
    "# image_file = \"checkerboard.png\"\n",
    "\n",
    "# color_img = cv2.imread(image_file)\n",
    "# img = cv2.cvtColor(color_img, cv2.COLOR_BGR2GRAY)\n",
    "# color_img = cv2.cvtColor(color_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# # Need non-free activated\n",
    "# sift = cv2.SIFT()\n",
    "# mask = np.uint8(np.ones(img.shape))\n",
    "# kp, des = sift.detectAndCompute(img, None)\n",
    "\n",
    "# color_img = cv2.drawKeypoints(color_img, kp, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "# plt.imshow(color_img)\n",
    "# print(len(corners), \"corners\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References ##\n",
    "\n",
    "[1] \"Introduction to SIFT (Scale-Invariant Feature Transform)\", https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_sift_intro/py_sift_intro.html#sift-intro\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
